{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook BYOL.ipynb to script\n",
      "[NbConvertApp] Writing 8063 bytes to BYOL.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da2986/anaconda3/lib/python3.9/site-packages/pl_bolts/utils/warnings.py:30: UserWarning: You want to use `gym` which is not installed yet, install it with `pip install gym`.\n",
      "  stdout_func(\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script BYOL.ipynb\n",
    "import BYOL\n",
    "import torch\n",
    "import time\n",
    "from pretrain_dataloader import *\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from utils import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import torch.nn.functional as F\n",
    "torch.backends.cudnn.enabled = True\n",
    "import copy\n",
    "import wandb\n",
    "import geoopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#returns necessary default arguments, training loader and networks\n",
    "args = return_default_args()\n",
    "args.batch_size = 256\n",
    "train_loader = prepare_cifar_train_loader(args)\n",
    "online = BYOL.euclidean_BYOL_module().to(device).to(memory_format=torch.channels_last)\n",
    "target = copy.deepcopy(online).to(device).to(memory_format=torch.channels_last)\n",
    "\n",
    "#gives us the mapping from fine labels to the coarse labels\n",
    "fine_to_coarse = fine_to_coarse_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "            {\"name\": \"backbone\", \"params\": online.network.parameters()},\n",
    "            {\n",
    "                \"name\": \"classifier\",\n",
    "                \"params\": online.classifier.parameters(),\n",
    "                \"lr\": args.classifier_lr,\n",
    "                \"weight_decay\": args.classifier_weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"coarse_classifier\",\n",
    "                \"params\": online.coarse_classifier.parameters(),\n",
    "                \"lr\": args.classifier_lr,\n",
    "                \"weight_decay\": args.classifier_weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"momentum_classifier\",\n",
    "                \"params\": target.classifier.parameters(),\n",
    "                \"lr\": args.classifier_lr,\n",
    "                \"weight_decay\": args.classifier_weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"coarse_momentum_classifier\",\n",
    "                \"params\": target.coarse_classifier.parameters(),\n",
    "                \"lr\": args.classifier_lr,\n",
    "                \"weight_decay\": args.classifier_weight_decay,\n",
    "            },\n",
    "            {'params': online.projector.parameters()},\n",
    "            {'params':online.predictor.parameters(),}\n",
    "        ]\n",
    "\n",
    "#in use if we are using a hyperbolic classifier\n",
    "if online.h_classifier:\n",
    "    print('Using Hyperbolic Classifier')\n",
    "    params.append({\n",
    "                \"name\": \"hyperbolic_classifier\",\n",
    "                \"params\": online.h_classifier.parameters(),\n",
    "                \"lr\": args.classifier_lr,\n",
    "                \"weight_decay\": args.classifier_weight_decay,\n",
    "            })\n",
    "    params.append({\n",
    "                \"name\": \"hyperbolic_coarse_classifier\",\n",
    "                \"params\": online.h_coarse_classifier.parameters(),\n",
    "                \"lr\": args.classifier_lr,\n",
    "                \"weight_decay\": args.classifier_weight_decay,\n",
    "            })\n",
    "    params.append({\n",
    "                \"name\": \"momentum_hyperbolic_classifier\",\n",
    "                \"params\": target.h_classifier.parameters(),\n",
    "                \"lr\": args.classifier_lr,\n",
    "                \"weight_decay\": args.classifier_weight_decay,\n",
    "            })\n",
    "    params.append({\n",
    "                \"name\": \"momentum_hyperbolic_coarse_classifier\",\n",
    "                \"params\": target.h_coarse_classifier.parameters(),\n",
    "                \"lr\": args.classifier_lr,\n",
    "                \"weight_decay\": args.classifier_weight_decay,\n",
    "            })\n",
    "    params.append({'params': online.h_representation.parameters()})\n",
    "    params.append({'params': online.h_representation_coarse.parameters()})\n",
    "    params.append({'params': online.hyperbolic_projector.parameters()})\n",
    "\n",
    "args.lr = .001\n",
    "args.wd = 1.5e-5\n",
    "\n",
    "#opt = geoopt.optim.RiemannianAdam(params, lr=args.lr, weight_decay=args.wd, stabilize=10)\n",
    "opt = torch.optim.AdamW(params, lr = args.lr, weight_decay = args.wd)\n",
    "schedule = LinearWarmupCosineAnnealingLR(\n",
    "                    opt,\n",
    "                    warmup_epochs= 10 * 195,\n",
    "                    max_epochs= 195 * 200,\n",
    "                    warmup_start_lr=3e-05,\n",
    "                    eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(data, online, target, fine_to_coarse = fine_to_coarse):\n",
    "    step_metrics = {}\n",
    "    \n",
    "    labels2 = []\n",
    "    for i in data[2]:\n",
    "        labels2.append(fine_to_coarse[i.item()])\n",
    "    labels2 = torch.Tensor(labels2)\n",
    "    labels2 = labels2.long().to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    online1 = online(data[1][0])\n",
    "    online2 = online(data[1][1])\n",
    "    #target1 = target.momentum_forward(data[1][0])\n",
    "    #target2 = target.momentum_forward(data[1][1])\n",
    "    \n",
    "    target1 = online1\n",
    "    target2 = online2\n",
    "    \n",
    "    #byol_loss\n",
    "    step_metrics[\"byol_loss\"] = byol_loss_func(online1['p'], target2['z'])\n",
    "    step_metrics[\"byol_loss\"] += byol_loss_func(online2['p'], target1['z'])\n",
    "    \n",
    "    #cross entropy loss\n",
    "    \n",
    "    step_metrics[\"online_cross_entropy_loss\"] = F.cross_entropy(online1['logits'], data[2], ignore_index=-1)\n",
    "    step_metrics[\"momentum_cross_entropy_loss\"] = F.cross_entropy(target1['logits'], data[2], ignore_index=-1)\n",
    "    step_metrics[\"coarse_online_cross_entropy_loss\"] = F.cross_entropy(online1['coarse_logits'], labels2, ignore_index=-1)\n",
    "    step_metrics[\"coarse_momentum_cross_entropy_loss\"] = F.cross_entropy(target1['coarse_logits'], labels2, ignore_index=-1)\n",
    "    \n",
    "\n",
    "    if online.h_classifier:\n",
    "        step_metrics[\"online_hyperbolic_cross_entropy_loss\"] = F.cross_entropy(online1['h_logits'], data[2], ignore_index=-1)\n",
    "        step_metrics[\"momentum_hyperbolic_cross_entropy_loss\"] = F.cross_entropy(target1['h_logits'], data[2], ignore_index=-1)\n",
    "        step_metrics[\"coarse_online_hyperbolic_cross_entropy_loss\"] = F.cross_entropy(online1['h_coarse_logits'], labels2, ignore_index=-1)\n",
    "        step_metrics[\"coarse_momentum_hyperbolic_cross_entropy_loss\"] = F.cross_entropy(target1['h_coarse_logits'], labels2, ignore_index=-1)\n",
    "    \n",
    "    #accuracy of predictions\n",
    "    _, predicted = torch.max(online1['logits'], 1)\n",
    "    step_metrics[\"online_acc1\"] = (predicted == data[2]).sum()\n",
    "    _, predicted = torch.max(target1['logits'], 1)\n",
    "    step_metrics[\"target_acc1\"] = (predicted == data[2]).sum()\n",
    "    \n",
    "    _, pred = online1['logits'].topk(5)\n",
    "    data2 = data[2].unsqueeze(1).expand_as(pred)\n",
    "    step_metrics[\"online_acc5\"] = (data2 == pred).any(dim = 1).sum()\n",
    "    _, pred = target1['logits'].topk(5)\n",
    "    step_metrics[\"target_acc5\"] = (data2 == pred).any(dim = 1).sum()\n",
    "    \n",
    "    #accuracy of predictions\n",
    "    _, predicted = torch.max(online1['coarse_logits'], 1)\n",
    "    step_metrics[\"coarse_online_acc1\"] = (predicted == labels2).sum()\n",
    "    _, predicted = torch.max(target1['coarse_logits'], 1)\n",
    "    step_metrics[\"coarse_target_acc1\"] = (predicted == labels2).sum()\n",
    "    \n",
    "    _, pred = online1['coarse_logits'].topk(5)\n",
    "    labels2_ = labels2.unsqueeze(1).expand_as(pred)\n",
    "    step_metrics[\"coarse_online_acc5\"] = (labels2_ == pred).any(dim = 1).sum()\n",
    "    _, pred = target1['coarse_logits'].topk(5)\n",
    "    step_metrics[\"coarse_target_acc5\"] = (labels2_ == pred).any(dim = 1).sum()\n",
    "    \n",
    "    if online.h_classifier:\n",
    "        #accuracy of predictions\n",
    "        _, predicted = torch.max(online1['h_logits'], 1)\n",
    "        step_metrics[\"online_hyperbolic_acc1\"] = (predicted == data[2]).sum()\n",
    "        _, predicted = torch.max(target1['h_logits'], 1)\n",
    "        step_metrics[\"target_hyperbolic_acc1\"] = (predicted == data[2]).sum()\n",
    "\n",
    "        _, pred = online1['h_logits'].topk(5)\n",
    "        data[2] = data[2].unsqueeze(1).expand_as(pred)\n",
    "        step_metrics[\"online_hyperbolic_acc5\"] = (data[2] == pred).any(dim = 1).sum()\n",
    "        _, pred = target1['h_logits'].topk(5)\n",
    "        step_metrics[\"target_hyperbolic_acc5\"] = (data[2] == pred).any(dim = 1).sum()\n",
    "\n",
    "        #accuracy of predictions\n",
    "        _, predicted = torch.max(online1['h_coarse_logits'], 1)\n",
    "        step_metrics[\"coarse_online_hyperbolic_acc1\"] = (predicted == labels2).sum()\n",
    "        _, predicted = torch.max(target1['h_coarse_logits'], 1)\n",
    "        step_metrics[\"coarse_target_hyperbolic_acc1\"] = (predicted == labels2).sum()\n",
    "\n",
    "        _, pred = online1['h_coarse_logits'].topk(5)\n",
    "        labels2 = labels2.unsqueeze(1).expand_as(pred)\n",
    "        step_metrics[\"coarse_online_hyperbolic_acc5\"] = (labels2 == pred).any(dim = 1).sum()\n",
    "        _, pred = target1['h_coarse_logits'].topk(5)\n",
    "        step_metrics[\"coarse_target_hyperbolic_acc5\"] = (labels2 == pred).any(dim = 1).sum()\n",
    "                   \n",
    "        \n",
    "    \n",
    "    \n",
    "    #metrics to track\n",
    "    with torch.no_grad():\n",
    "        online1p_softmax = F.softmax(online1['p'], dim = 1)\n",
    "        online2p_softmax = F.softmax(online2['p'], dim = 1)\n",
    "        online1rep_softmax = F.softmax(online1['Representation'], dim = 1)\n",
    "        online2rep_softmax = F.softmax(online2['Representation'], dim = 1)\n",
    "        online1z_softmax = F.softmax(online1['z'], dim = 1)\n",
    "        online2z_softmax = F.softmax(online2['z'], dim = 1)\n",
    "        \n",
    "        target1z_softmax = F.softmax(target1['z'], dim = 1)\n",
    "        target2z_softmax = F.softmax(target1['z'], dim = 1)\n",
    "        target1rep_softmax = F.softmax(target1['Representation'], dim = 1)\n",
    "        target2rep_softmax = F.softmax(target2['Representation'], dim = 1)\n",
    "        \n",
    "        \n",
    "        cross_entropy = F.cross_entropy(online1p_softmax, target2z_softmax, ignore_index=-1) + F.cross_entropy(online2p_softmax, target1z_softmax, ignore_index=-1)\n",
    "        l1_dist = F.l1_loss(online1['p'], target2['z']) + F.l1_loss(online2['p'], target1['z'])\n",
    "        l2_dist = F.mse_loss(online1['p'], target2['z']) + F.mse_loss(online2['p'], target1['z'])\n",
    "        smooth_l1 = F.smooth_l1_loss(online1['p'], target2['z']) + F.smooth_l1_loss(online2['p'], target1['z'])\n",
    "        kl_div = F.kl_div(online1p_softmax, target2z_softmax) + F.kl_div(online2p_softmax, target1z_softmax)\n",
    "        \n",
    "        representation_l1 = F.l1_loss(online1['Representation'], target2['Representation']) + F.l1_loss(online2['Representation'], target1['Representation'])\n",
    "        representation_l2 = F.mse_loss(online1['Representation'], target2['Representation']) + F.mse_loss(online2['Representation'], target1['Representation'])\n",
    "        representation_cross_entropy = F.cross_entropy(online1rep_softmax, target2rep_softmax, ignore_index=-1) + F.cross_entropy(online2rep_softmax, target1rep_softmax, ignore_index=-1)\n",
    "        representation_kl = F.kl_div(online1rep_softmax, target2rep_softmax) + F.kl_div(online2rep_softmax, target1rep_softmax)\n",
    "        representation_cos_sim = F.cosine_similarity(online1['Representation'], target2['Representation']) + F.cosine_similarity(online2['Representation'], target1['Representation'])\n",
    "\n",
    "        projection_l1 = F.l1_loss(online1['z'], target2['z']) + F.l1_loss(online2['z'], target1['z'])\n",
    "        projection_l2 = F.mse_loss(online1['z'], target2['z']) + F.mse_loss(online2['z'], target1['z'])\n",
    "        projection_cross_entropy = F.cross_entropy(online1z_softmax, target2z_softmax, ignore_index=-1) + F.cross_entropy(online2z_softmax, target1z_softmax, ignore_index=-1)\n",
    "        projection_kl = F.kl_div(online1z_softmax, target2z_softmax) + F.kl_div(online2z_softmax, target1z_softmax)\n",
    "        projection_cos_sim = F.cosine_similarity(online1['z'], target2['z']) + F.cosine_similarity(online2['z'], target1['z'])\n",
    "\n",
    "        momentum_projection_cos_sim = F.cosine_similarity(target1['z'], target2['z'])\n",
    "        momentum_representation_cos_sim = F.cosine_similarity(target1['Representation'], target2['Representation'])\n",
    "        momentum_representation_l2 =  F.mse_loss(target1['Representation'], target2['Representation'])\n",
    "        momentum_projection_l2 =  F.mse_loss(target1['z'], target2['z'])\n",
    "\n",
    "        online_projection_cos_sim = F.cosine_similarity(online1['z'], online2['z'])\n",
    "        online_representation_cos_sim = F.cosine_similarity(online1['Representation'], online2['Representation'])\n",
    "        online_representation_l2 =  F.mse_loss(online1['Representation'], online2['Representation'])\n",
    "        online_projection_l2 =  F.mse_loss(online1['z'], online2['z'])\n",
    "        \n",
    "        online_representation_std = torch.mean(torch.std(online1['Representation']))\n",
    "        online_projection_std = torch.mean(torch.std(online1['z']))\n",
    "        online_prediction_std = torch.mean(torch.std(online1['p']))\n",
    "        target_representation_std = torch.mean(torch.std(target1['Representation']))\n",
    "        target_projection_std = torch.mean(torch.std(target1['z']))\n",
    "\n",
    "        step_metrics.update({\n",
    "            \"train_feats_cross_entropy\": cross_entropy,\n",
    "            \"train_feats_l1_dist\": l1_dist,\n",
    "            \"train_feats_l2_dist\": l2_dist,\n",
    "            \"train_feats_smooth_l1\": smooth_l1,\n",
    "            \"train_feats_kl_div\": kl_div,\n",
    "            \"representation_l1\": representation_l1,\n",
    "            \"representation_l2\": representation_l1,\n",
    "            \"representation_cross_entropy\": representation_cross_entropy,\n",
    "            \"representation_kl\": representation_kl,\n",
    "            \"representation_cos_sim\": representation_cos_sim.mean(),\n",
    "            \"projection_l1\": projection_l1,\n",
    "            \"projection_l2\": projection_l2,\n",
    "            \"projection_cross_entropy\": projection_cross_entropy,\n",
    "            \"projection_kl\": projection_kl,\n",
    "            \"projection_cos_sim\": projection_cos_sim.mean(),\n",
    "            \"momentum_projection_cos_sim\": momentum_projection_cos_sim.mean(),\n",
    "            \"momentum_representation_cos_sim\": momentum_representation_cos_sim.mean(),\n",
    "            \"momentum_representation_l2\": momentum_representation_l2,\n",
    "            \"momentum_representation_l2\": momentum_projection_l2,\n",
    "            \"online_projection_cos_sim\": online_projection_cos_sim.mean(),\n",
    "            \"online_representation_cos_sim\": online_representation_cos_sim.mean(),\n",
    "            \"online_representation_l2\": online_representation_l2,\n",
    "            \"online_projection_l2\": online_projection_l2,\n",
    "            \"online_representation_std\": online_representation_std,\n",
    "            \"online_projection_std\": online_projection_std,\n",
    "            \"online_prediction_std\": online_prediction_std,\n",
    "            \"target_representation_std\": target_representation_std,\n",
    "            \"target_projection_std\": target_projection_std,\n",
    "            })\n",
    "        \n",
    "    loss = step_metrics[\"byol_loss\"] + step_metrics[\"online_cross_entropy_loss\"] + \\\n",
    "            step_metrics[\"momentum_cross_entropy_loss\"] + step_metrics[\"coarse_online_cross_entropy_loss\"] \\\n",
    "            + step_metrics[\"coarse_momentum_cross_entropy_loss\"]\n",
    "    \n",
    "    if online.h_classifier:\n",
    "        loss += step_metrics[\"online_hyperbolic_cross_entropy_loss\"] + step_metrics[\"momentum_hyperbolic_cross_entropy_loss\"] + \\\n",
    "                step_metrics[\"coarse_online_hyperbolic_cross_entropy_loss\"] + step_metrics[\"coarse_momentum_hyperbolic_cross_entropy_loss\"]\n",
    "        \n",
    "    return loss, step_metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdcaustin33\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/da2986/hyperbolic_byol/wandb/run-20220601_235640-1t1h9ypr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dcaustin33/hyperbolic_byol/runs/1t1h9ypr\" target=\"_blank\">BYOL Hyperbolic No Linear Before H-Classifier</a></strong> to <a href=\"https://wandb.ai/dcaustin33/hyperbolic_byol\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = True\n",
    "name = 'BYOL Hyperbolic No Linear Before H-Classifier'\n",
    "if log:\n",
    "    wandb.init(config = args.__dict__, name = name, project = 'hyperbolic_byol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 43.83 15.93349859775641 1.376688482822516 0.03812 0.03812 0.11544\n",
      "1 43.08 15.673069411057693 1.2910663311298076 0.04546 0.04546 0.13044\n",
      "2 43.39 15.549864783653845 1.2780930739182692 0.05118 0.05118 0.14302\n",
      "3 43.32 15.51539087540064 1.268421427408854 0.0579 0.0579 0.15096\n",
      "4 43.41 15.468130258413462 1.227275124574319 0.06006 0.06006 0.1557\n",
      "5 43.4 15.378087439903846 1.1650713798327323 0.06488 0.06488 0.1643\n",
      "6 43.52 15.24981219951923 1.1086284930889423 0.07216 0.07216 0.1681\n",
      "7 43.12 15.364014923878205 1.0953783084184696 0.07 0.07 0.16836\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "if online.h_classifier:\n",
    "    print('Running Hyperbolic version of BYOL')\n",
    "    \n",
    "for e in range(200):\n",
    "    metrics = {\n",
    "        \"byol_loss\": 0,\n",
    "        'online_cross_entropy_loss': 0,\n",
    "        'momentum_cross_entropy_loss': 0,\n",
    "        'coarse_online_cross_entropy_loss': 0,\n",
    "        'coarse_momentum_cross_entropy_loss': 0,\n",
    "        'online_acc1': 0,\n",
    "        'target_acc1': 0,\n",
    "        'online_acc5': 0,\n",
    "        'target_acc5': 0,\n",
    "        'coarse_online_acc1': 0,\n",
    "        'coarse_target_acc1': 0,\n",
    "        'coarse_online_acc5': 0,\n",
    "        'coarse_target_acc5': 0,\n",
    "        \"train_feats_cross_entropy\": 0,\n",
    "        \"train_feats_l1_dist\": 0,\n",
    "        \"train_feats_l2_dist\": 0,\n",
    "        \"train_feats_smooth_l1\": 0,\n",
    "        \"train_feats_kl_div\": 0,\n",
    "        \"representation_l1\": 0,\n",
    "        \"representation_l2\": 0,\n",
    "        \"representation_cross_entropy\": 0,\n",
    "        \"representation_kl\": 0,\n",
    "        \"representation_cos_sim\": 0,\n",
    "        \"projection_l1\": 0,\n",
    "        \"projection_l2\": 0,\n",
    "        \"projection_cross_entropy\": 0,\n",
    "        \"projection_kl\": 0,\n",
    "        \"projection_cos_sim\": 0,\n",
    "        \"momentum_projection_cos_sim\": 0,\n",
    "        \"momentum_representation_cos_sim\": 0,\n",
    "        \"momentum_representation_l2\": 0,\n",
    "        \"momentum_representation_l2\": 0,\n",
    "        \"online_projection_cos_sim\": 0,\n",
    "        \"online_representation_cos_sim\": 0,\n",
    "        \"online_representation_l2\": 0,\n",
    "        \"online_projection_l2\": 0,\n",
    "        \"online_representation_std\": 0,\n",
    "        \"online_projection_std\": 0,\n",
    "        \"online_prediction_std\": 0,\n",
    "        \"target_representation_std\": 0,\n",
    "        \"target_projection_std\": 0,\n",
    "        }\n",
    "    \n",
    "    if online.h_classifier:\n",
    "        metrics['online_hyperbolic_cross_entropy_loss'] = 0\n",
    "        metrics['momentum_hyperbolic_cross_entropy_loss'] = 0\n",
    "        metrics['coarse_online_hyperbolic_cross_entropy_loss'] = 0\n",
    "        metrics['coarse_momentum_hyperbolic_cross_entropy_loss'] = 0\n",
    "        \n",
    "        metrics['online_hyperbolic_acc1'] = 0\n",
    "        metrics['target_hyperbolic_acc1'] = 0\n",
    "        metrics['online_hyperbolic_acc5'] = 0\n",
    "        metrics['target_hyperbolic_acc5'] = 0\n",
    "        metrics['coarse_online_hyperbolic_acc1'] = 0\n",
    "        metrics['coarse_target_hyperbolic_acc1'] = 0\n",
    "        metrics['coarse_online_hyperbolic_acc5'] = 0\n",
    "        metrics['coarse_target_hyperbolic_acc5'] = 0\n",
    "        \n",
    "    \n",
    "    \n",
    "    total_loss = 0\n",
    "    now = time.time()\n",
    "    for i_, data in enumerate(train_loader):\n",
    "        \n",
    "        with torch.autocast(device):\n",
    "            step += 1\n",
    "            data[0] = data[0].to(device)\n",
    "            data[1][0] = data[1][0].to(device).to(memory_format=torch.channels_last)\n",
    "            data[1][1] = data[1][1].to(device).to(memory_format=torch.channels_last)\n",
    "            data[2] = data[2].to(device)\n",
    "            \n",
    "            \n",
    "            loss, step_metrics = training_step(data, online, target)\n",
    "            opt.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(online.parameters(), 1.0)\n",
    "            scaler.step(opt)\n",
    "            schedule.step()\n",
    "            scaler.update()\n",
    "            update_target_params(online.network.parameters(), target.network.parameters(), .99)\n",
    "            update_target_params(online.projector.parameters(), target.projector.parameters(), .99)\n",
    "                \n",
    "        total_loss += loss\n",
    "        for key in step_metrics:\n",
    "            metrics[key] += step_metrics[key]\n",
    "    \n",
    "    print(e, round(time.time() - now, 2), total_loss.item() / (i_+1), metrics['byol_loss'].item() / (i_+1), \n",
    "          metrics['online_acc1'].item() / 50000, metrics['target_acc1'].item() / 50000, metrics['coarse_online_acc1'].item() / 50000)\n",
    "    if log:\n",
    "        i_ += 1\n",
    "        for key in metrics:\n",
    "            if key[-4:-1] == 'acc':\n",
    "                metrics[key] = metrics[key] / 50000\n",
    "            else:\n",
    "                metrics[key] = metrics[key]/i_\n",
    "        wandb.log(metrics)\n",
    "    checkpoint = {\n",
    "    'online': online.state_dict(),\n",
    "    'target': target.state_dict(),\n",
    "    'epoch': e,\n",
    "    'optimizer': opt.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, 'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.01 LR\n",
    "'''\n",
    "0 54.98 16.301710236378206 1.3990733611278046 0.02588 0.02302 0.09818\n",
    "1 53.84 15.126978165064102 0.9789630596454327 0.05156 0.05114 0.14322\n",
    "2 54.21 14.806014623397436 0.931276370317508 0.0646 0.06716 0.1626\n",
    "3 54.14 14.602803235176282 0.8721234443860176 0.07308 0.0747 0.17544\n",
    "4 54.28 14.423293519631411 0.8290852864583333 0.08204 0.08572 0.18862\n",
    "5 54.18 14.334297375801283 0.8047475179036458 0.09012 0.09398 0.19864\n",
    "6 54.06 14.161805138221155 0.792535165640024 0.09704 0.10206 0.2092\n",
    "7 54.09 14.14975335536859 0.7905127892127404 0.1046 0.10946 0.21614\n",
    "8 54.09 13.999675731169871 0.7640458327073317 0.1108 0.11748 0.22088\n",
    "9 53.94 13.861602313701923 0.7397880358573717 0.1194 0.12544 0.23064\n",
    "10 54.06 13.705500050080127 0.7379991580278445 0.12708 0.13374 0.24136\n",
    "11 53.99 13.454622395833333 0.7214473626552484 0.1366 0.14262 0.2535\n",
    "12 54.06 13.020966045673077 0.7090653639573318 0.15088 0.1568 0.26708\n",
    "13 53.97 12.838525390625 0.7102927183493589 0.1607 0.16826 0.27686'''\n",
    "\n",
    "#lr = .1 got nana in the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(online.state_dict(), name)\n",
    "torch.save(target.state_dict(), 'target' + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Representation': tensor([[1.0902, 0.7197, 0.7068,  ..., 0.6889, 0.6700, 0.7505],\n",
       "         [0.6945, 0.7999, 0.8918,  ..., 1.0819, 0.9804, 0.8990]],\n",
       "        grad_fn=<ReshapeAliasBackward0>),\n",
       " 'logits': tensor([[-1.0778e-01,  6.7949e-01,  2.9006e-01, -5.4419e-01,  8.8811e-01,\n",
       "          -6.3720e-01, -3.0231e-01,  4.0207e-01,  6.0091e-01, -1.1101e+00,\n",
       "          -2.7813e-01,  4.0618e-01,  5.7357e-01,  8.0544e-01,  3.3681e-01,\n",
       "          -3.4905e-01,  5.9667e-01, -7.6058e-01, -2.2123e-01, -1.1967e+00,\n",
       "          -2.1913e-01,  3.6329e-01, -4.0279e-01, -3.3254e-01, -3.8375e-02,\n",
       "          -1.3133e-01,  5.0848e-01,  1.9677e-01,  2.6188e-01, -5.9601e-01,\n",
       "           4.1806e-01,  1.0126e-01,  4.5833e-02, -1.4168e-01,  1.9744e-01,\n",
       "           4.5370e-01,  4.6671e-01,  8.5084e-01,  2.1330e-01, -9.1768e-01,\n",
       "          -1.0758e+00,  7.0109e-01, -1.5537e-01,  6.2724e-02,  1.3873e+00,\n",
       "          -7.1104e-02, -7.2754e-02,  1.0333e+00, -4.2682e-01, -1.0542e-01,\n",
       "           1.2579e-01, -1.3975e-01,  4.2783e-01, -1.6718e-01,  8.6556e-01,\n",
       "           2.6752e-01,  4.8627e-02,  5.8114e-01, -3.2771e-01,  2.1587e-01,\n",
       "           1.1518e-01,  3.2449e-01, -2.4092e-01, -2.2531e-01,  6.1490e-01,\n",
       "           1.1473e-01, -4.2295e-01,  1.2853e-01,  3.3619e-02,  4.0828e-01,\n",
       "          -8.0600e-01,  5.0701e-01, -3.5777e-01,  5.3796e-01,  1.3452e-01,\n",
       "          -2.8786e-01,  5.0501e-01,  4.6791e-01, -2.4054e-01, -3.0992e-01,\n",
       "          -2.8312e-01,  7.7540e-01,  4.9865e-01, -2.0992e-01,  9.1911e-02,\n",
       "          -4.7188e-01,  1.3197e+00, -3.8866e-01,  6.2927e-02,  1.2825e-01,\n",
       "           4.6278e-01,  1.2951e-01,  1.2219e-01,  5.2271e-01,  4.5976e-05,\n",
       "           4.7650e-01,  2.4949e-01,  5.2577e-03,  7.1669e-02, -1.9146e-01],\n",
       "         [-2.3534e-02,  5.8700e-01,  2.7871e-01, -5.0583e-01,  8.0708e-01,\n",
       "          -8.0225e-01, -3.9088e-01,  3.4334e-01,  7.0536e-01, -8.7677e-01,\n",
       "          -2.4663e-01,  3.9243e-01,  2.6695e-01,  8.6522e-01,  1.4351e-01,\n",
       "          -1.3385e-01,  7.2991e-01, -6.4747e-01, -4.4657e-02, -9.8339e-01,\n",
       "          -1.7755e-01,  5.0591e-01, -4.8503e-01, -3.2655e-01, -1.6290e-01,\n",
       "          -1.4123e-02,  6.8911e-01, -1.1501e-01,  4.5729e-01, -7.9049e-01,\n",
       "           3.0267e-01,  3.9540e-01,  2.3170e-01,  1.3255e-02,  1.6586e-01,\n",
       "           6.5148e-01,  5.2523e-01,  8.7785e-01,  3.1109e-01, -1.0859e+00,\n",
       "          -1.1656e+00,  8.3250e-01, -2.8100e-01,  4.2558e-03,  1.6183e+00,\n",
       "           7.7193e-02, -1.2952e-01,  1.2636e+00, -4.9329e-01,  4.5775e-02,\n",
       "           2.2979e-01, -3.3640e-01,  6.1638e-01,  3.0498e-02,  9.5594e-01,\n",
       "           1.8179e-01,  1.7042e-02,  8.5924e-01, -4.0372e-01, -3.9314e-02,\n",
       "          -8.3540e-02,  1.7041e-01, -1.2390e-01, -2.4861e-01,  7.2158e-01,\n",
       "           4.0727e-01, -3.5175e-01,  2.4073e-01, -1.4286e-01,  2.9568e-01,\n",
       "          -1.0531e+00,  5.4357e-01, -5.7197e-01,  7.6503e-01, -1.9559e-01,\n",
       "          -1.8661e-01,  1.9031e-01,  5.1595e-01, -3.5886e-01, -3.1327e-01,\n",
       "          -4.1690e-01,  9.3954e-01,  6.7352e-01, -4.1541e-01,  1.2431e-01,\n",
       "          -4.1239e-01,  1.3298e+00, -3.6742e-01, -1.5297e-01,  7.6327e-02,\n",
       "           6.1986e-01,  3.2491e-01, -3.3082e-01,  4.9156e-01, -1.3024e-01,\n",
       "           6.1837e-01,  1.5081e-01,  8.8181e-02,  3.3190e-01, -1.9409e-01]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'coarse_logits': tensor([[-0.9396, -0.3408, -0.2001, -0.2514, -0.2164, -0.8491, -0.2933, -0.2584,\n",
       "          -0.1323,  1.1104, -0.0818,  0.1520,  0.6462, -0.5322,  1.0850,  0.1641,\n",
       "          -0.2150,  0.4617,  0.0470,  0.4295],\n",
       "         [-0.7083, -0.1830, -0.2700, -0.2341, -0.2871, -0.9902, -0.3176, -0.2118,\n",
       "          -0.1677,  1.1075,  0.1207,  0.5615,  0.7864, -0.4739,  1.0722,  0.2589,\n",
       "          -0.1574,  0.6085,  0.1741,  0.5815]], grad_fn=<AddmmBackward0>),\n",
       " 'h_logits': tensor([[-9.2845e-02,  1.7918e-02,  2.4724e-02, -2.1277e-02,  4.6425e-02,\n",
       "           6.4973e-03,  1.4007e-02,  5.8788e-05,  6.0601e-02, -1.0166e-01,\n",
       "           3.3631e-02,  2.2528e-02, -3.0607e-03,  5.9924e-03, -3.7413e-03,\n",
       "          -3.3319e-02, -6.0853e-02,  1.3117e-01,  5.7345e-03,  7.4860e-03,\n",
       "           5.5468e-02,  2.6965e-02,  4.5844e-02,  2.6752e-03, -5.7804e-02,\n",
       "           7.6061e-04,  6.7870e-02,  5.7303e-02,  2.2709e-02,  1.9993e-02,\n",
       "           3.0640e-03,  8.6461e-03,  2.1057e-02,  1.5569e-03, -3.5902e-02,\n",
       "          -2.6143e-02,  3.9474e-02, -5.8183e-02, -3.6075e-02, -5.1067e-02,\n",
       "          -2.5189e-02,  2.9830e-02, -1.0565e-01, -2.1631e-02, -4.7621e-03,\n",
       "          -2.7780e-02, -5.0943e-03,  4.9918e-02,  7.3118e-03, -7.6280e-02,\n",
       "          -1.5644e-04,  5.2768e-03, -4.8023e-02,  1.3395e-03,  1.7473e-02,\n",
       "           7.1321e-02, -3.8246e-02,  1.2681e-02, -3.7885e-02,  4.6546e-03,\n",
       "           7.3833e-02,  4.9951e-02,  8.2073e-03, -1.8521e-02, -2.8064e-03,\n",
       "           3.4910e-02, -3.3577e-02,  1.8590e-02, -1.6260e-02, -8.0267e-03,\n",
       "          -3.6381e-02, -3.8588e-02, -1.7414e-02,  5.0319e-02,  6.4508e-02,\n",
       "           1.4537e-02,  5.0035e-02,  5.7564e-02,  8.8456e-05,  2.5527e-02,\n",
       "          -1.7078e-02, -4.9599e-03,  3.3234e-02, -9.7853e-02, -1.1689e-02,\n",
       "          -2.8308e-02,  1.4475e-02, -2.0903e-02,  1.8975e-02,  3.8623e-02,\n",
       "           1.2055e-02,  2.6674e-02,  5.4065e-02,  1.8392e-02,  2.4000e-02,\n",
       "          -6.9728e-02,  9.3292e-02,  2.3306e-03, -5.6429e-02, -3.9192e-02],\n",
       "         [-8.3879e-02,  1.4408e-02,  2.6453e-02, -2.6736e-02,  5.8630e-02,\n",
       "           2.2157e-02,  1.2283e-02,  6.9892e-03,  4.2484e-02, -6.8168e-02,\n",
       "           3.6347e-02,  2.3838e-02, -1.1594e-02, -2.4430e-03,  4.3975e-03,\n",
       "          -2.9007e-02, -8.3052e-02,  1.2489e-01, -7.7005e-04,  5.3573e-03,\n",
       "           4.4742e-02,  1.4979e-02,  2.8121e-02, -2.1368e-02, -5.9498e-02,\n",
       "           2.6411e-03,  9.3901e-02,  3.9178e-02,  1.6574e-02,  8.8293e-03,\n",
       "           1.1344e-02,  1.1893e-02,  5.5208e-02, -6.7701e-04, -4.2212e-02,\n",
       "          -5.0764e-02,  4.2801e-02, -7.9953e-02, -7.6243e-02, -4.1119e-02,\n",
       "          -1.8574e-02,  1.5961e-02, -9.6154e-02,  8.4227e-03,  2.8238e-02,\n",
       "          -3.6695e-02, -2.4998e-02,  5.3774e-02,  1.7785e-02, -7.8148e-02,\n",
       "          -1.4301e-02,  1.9611e-02, -3.6767e-02,  7.1491e-03, -2.1525e-03,\n",
       "           9.5023e-02, -5.2829e-02, -3.5422e-03, -3.0405e-02, -2.0949e-02,\n",
       "           5.6540e-02,  3.9101e-02, -4.1307e-03, -1.4585e-02,  8.3206e-03,\n",
       "           5.7744e-02, -4.5109e-02,  1.5269e-02, -3.1999e-02, -2.2052e-02,\n",
       "          -8.6035e-04, -3.5279e-02, -2.1992e-02,  3.5050e-02,  5.6200e-02,\n",
       "           4.0973e-02,  5.0171e-02,  4.3044e-02, -5.8846e-03,  3.7042e-02,\n",
       "          -1.0245e-02,  9.8326e-03,  5.0890e-02, -7.2183e-02, -8.6209e-03,\n",
       "          -3.1429e-02,  3.8657e-02,  2.6679e-03, -2.0422e-02,  1.7894e-02,\n",
       "           5.8690e-02,  2.8516e-02,  5.3637e-02,  4.0092e-02,  4.1353e-03,\n",
       "          -5.7828e-02,  7.8584e-02,  3.1548e-05, -3.9536e-02, -4.9886e-02]],\n",
       "        dtype=torch.float64, grad_fn=<MulBackward0>),\n",
       " 'h_coarse_logits': tensor([[-0.0052, -0.0028,  0.0355, -0.0024,  0.0111, -0.0168,  0.0173,  0.1157,\n",
       "           0.0792, -0.0471,  0.0141,  0.0096,  0.0261, -0.0282, -0.0014,  0.0163,\n",
       "          -0.0135, -0.0172, -0.0084, -0.0059],\n",
       "         [-0.0254, -0.0126,  0.0274,  0.0016,  0.0075, -0.0180,  0.0172,  0.1079,\n",
       "           0.0979, -0.0610,  0.0246, -0.0066,  0.0438, -0.0452, -0.0245,  0.0393,\n",
       "           0.0062, -0.0257,  0.0044,  0.0086]], dtype=torch.float64,\n",
       "        grad_fn=<MulBackward0>),\n",
       " 'z': tensor([[0.0000e+00, 2.1944e-03, 0.0000e+00, 9.5826e-04, 0.0000e+00, 4.1924e-03,\n",
       "          1.8219e-03, 1.1929e-03, 3.9934e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.8671e-03, 0.0000e+00, 0.0000e+00, 6.9548e-04, 0.0000e+00, 1.6323e-04,\n",
       "          0.0000e+00, 0.0000e+00, 1.2323e-03, 4.0688e-03, 0.0000e+00, 3.7969e-03,\n",
       "          0.0000e+00, 0.0000e+00, 2.7288e-03, 2.4881e-04, 1.6838e-03, 9.1824e-04,\n",
       "          0.0000e+00, 1.1803e-03, 3.4343e-03, 2.4513e-03, 2.5761e-03, 2.6253e-04,\n",
       "          0.0000e+00, 0.0000e+00, 1.5170e-03, 0.0000e+00, 0.0000e+00, 5.2468e-03,\n",
       "          1.7841e-03, 2.2771e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8942e-03,\n",
       "          3.4338e-03, 0.0000e+00, 0.0000e+00, 2.0112e-03, 0.0000e+00, 1.8735e-03,\n",
       "          0.0000e+00, 0.0000e+00, 3.7695e-03, 0.0000e+00, 6.9937e-04, 2.1255e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0135e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2526e-03, 0.0000e+00,\n",
       "          0.0000e+00, 6.0600e-04, 0.0000e+00, 6.4405e-03, 1.4883e-03, 0.0000e+00,\n",
       "          2.5004e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4833e-04, 3.2743e-04,\n",
       "          6.1091e-03, 0.0000e+00, 4.5234e-03, 1.2376e-03, 1.1668e-03, 6.7150e-04,\n",
       "          2.4120e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7433e-03, 1.0084e-03,\n",
       "          8.3250e-04, 1.1290e-04, 1.9494e-04, 2.4628e-03, 3.7703e-03, 0.0000e+00,\n",
       "          4.6404e-03, 0.0000e+00, 1.4925e-03, 0.0000e+00, 4.2242e-03, 3.9054e-03,\n",
       "          1.7014e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1621e-04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2100e-03, 0.0000e+00, 3.0367e-04,\n",
       "          0.0000e+00, 4.2249e-04, 3.3492e-03, 6.1825e-03, 1.3669e-03, 4.0181e-04,\n",
       "          2.2856e-03, 0.0000e+00, 1.5034e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.3452e-03, 2.3596e-03, 3.5170e-03, 9.7925e-04, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 9.3207e-05, 0.0000e+00, 9.9160e-04, 0.0000e+00,\n",
       "          0.0000e+00, 1.9859e-03, 0.0000e+00, 2.2143e-03, 7.2772e-05, 4.1466e-03,\n",
       "          0.0000e+00, 0.0000e+00, 5.0652e-03, 5.7986e-03, 4.8916e-03, 2.4145e-03,\n",
       "          0.0000e+00, 1.9978e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4284e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2389e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.9203e-03, 0.0000e+00, 0.0000e+00, 1.0413e-03,\n",
       "          1.9335e-03, 3.4751e-03, 0.0000e+00, 2.5625e-03, 0.0000e+00, 3.9417e-03,\n",
       "          0.0000e+00, 0.0000e+00, 3.5119e-03, 0.0000e+00, 5.8110e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2401e-03,\n",
       "          2.3227e-03, 3.0595e-03, 5.9571e-03, 4.6725e-03, 0.0000e+00, 1.0846e-03,\n",
       "          0.0000e+00, 9.8593e-04, 0.0000e+00, 4.1479e-03, 4.1458e-03, 0.0000e+00,\n",
       "          0.0000e+00, 2.4384e-04, 0.0000e+00, 0.0000e+00, 4.3380e-03, 0.0000e+00,\n",
       "          2.4904e-03, 3.2522e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1643e-03,\n",
       "          1.2498e-03, 1.2000e-04, 0.0000e+00, 2.2218e-03, 0.0000e+00, 2.5388e-03,\n",
       "          0.0000e+00, 1.6072e-04, 0.0000e+00, 1.2290e-03, 2.3966e-03, 0.0000e+00,\n",
       "          7.6939e-04, 0.0000e+00, 0.0000e+00, 3.2007e-04, 0.0000e+00, 1.5154e-03,\n",
       "          0.0000e+00, 0.0000e+00, 4.7048e-03, 1.6991e-03, 0.0000e+00, 3.9463e-04,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.4263e-03, 0.0000e+00, 0.0000e+00, 2.6328e-03, 3.6871e-03,\n",
       "          1.8635e-03, 0.0000e+00, 1.1744e-03, 2.7540e-03],\n",
       "         [0.0000e+00, 2.1944e-03, 0.0000e+00, 9.5826e-04, 0.0000e+00, 4.1924e-03,\n",
       "          1.8219e-03, 1.1929e-03, 3.9927e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.8671e-03, 0.0000e+00, 0.0000e+00, 6.9547e-04, 0.0000e+00, 1.6325e-04,\n",
       "          0.0000e+00, 0.0000e+00, 1.2323e-03, 4.0688e-03, 0.0000e+00, 3.7969e-03,\n",
       "          0.0000e+00, 0.0000e+00, 2.7288e-03, 2.4881e-04, 1.6838e-03, 9.1826e-04,\n",
       "          0.0000e+00, 1.1803e-03, 3.4343e-03, 2.4513e-03, 2.5762e-03, 2.6252e-04,\n",
       "          0.0000e+00, 0.0000e+00, 1.5170e-03, 0.0000e+00, 0.0000e+00, 5.2468e-03,\n",
       "          1.7841e-03, 2.2771e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8942e-03,\n",
       "          3.4338e-03, 0.0000e+00, 0.0000e+00, 2.0112e-03, 0.0000e+00, 1.8735e-03,\n",
       "          0.0000e+00, 0.0000e+00, 3.7695e-03, 0.0000e+00, 6.9935e-04, 2.1255e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0135e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2526e-03, 0.0000e+00,\n",
       "          0.0000e+00, 6.0601e-04, 0.0000e+00, 6.4405e-03, 1.4884e-03, 0.0000e+00,\n",
       "          2.5004e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4832e-04, 3.2742e-04,\n",
       "          6.1091e-03, 0.0000e+00, 4.5234e-03, 1.2376e-03, 1.1667e-03, 6.7150e-04,\n",
       "          2.4119e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7433e-03, 1.0084e-03,\n",
       "          8.3249e-04, 1.1291e-04, 1.9493e-04, 2.4628e-03, 3.7703e-03, 0.0000e+00,\n",
       "          4.6404e-03, 0.0000e+00, 1.4925e-03, 0.0000e+00, 4.2242e-03, 3.9054e-03,\n",
       "          1.7014e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1621e-04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2099e-03, 0.0000e+00, 3.0365e-04,\n",
       "          0.0000e+00, 4.2249e-04, 3.3492e-03, 6.1825e-03, 1.3669e-03, 4.0181e-04,\n",
       "          2.2856e-03, 0.0000e+00, 1.5034e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.3452e-03, 2.3596e-03, 3.5170e-03, 9.7925e-04, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 9.3213e-05, 0.0000e+00, 9.9160e-04, 0.0000e+00,\n",
       "          0.0000e+00, 1.9859e-03, 0.0000e+00, 2.2143e-03, 7.2775e-05, 4.1466e-03,\n",
       "          0.0000e+00, 0.0000e+00, 5.0652e-03, 5.7986e-03, 4.8916e-03, 2.4145e-03,\n",
       "          0.0000e+00, 1.9978e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4284e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2389e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.9203e-03, 0.0000e+00, 0.0000e+00, 1.0412e-03,\n",
       "          1.9335e-03, 3.4752e-03, 0.0000e+00, 2.5625e-03, 0.0000e+00, 3.9417e-03,\n",
       "          0.0000e+00, 0.0000e+00, 3.5119e-03, 0.0000e+00, 5.8110e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2400e-03,\n",
       "          2.3227e-03, 3.0595e-03, 5.9571e-03, 4.6725e-03, 0.0000e+00, 1.0846e-03,\n",
       "          0.0000e+00, 9.8594e-04, 0.0000e+00, 4.1479e-03, 4.1458e-03, 0.0000e+00,\n",
       "          0.0000e+00, 2.4384e-04, 0.0000e+00, 0.0000e+00, 4.3380e-03, 0.0000e+00,\n",
       "          2.4905e-03, 3.2523e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1643e-03,\n",
       "          1.2497e-03, 1.2000e-04, 0.0000e+00, 2.2218e-03, 0.0000e+00, 2.5388e-03,\n",
       "          0.0000e+00, 1.6071e-04, 0.0000e+00, 1.2290e-03, 2.3967e-03, 0.0000e+00,\n",
       "          7.6939e-04, 0.0000e+00, 0.0000e+00, 3.2006e-04, 0.0000e+00, 1.5154e-03,\n",
       "          0.0000e+00, 0.0000e+00, 4.7048e-03, 1.6992e-03, 0.0000e+00, 3.9460e-04,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.4263e-03, 0.0000e+00, 0.0000e+00, 2.6328e-03, 3.6871e-03,\n",
       "          1.8635e-03, 0.0000e+00, 1.1744e-03, 2.7540e-03]],\n",
       "        grad_fn=<DifferentiableGraphBackward>),\n",
       " 'p': tensor([[0.0000e+00, 5.5985e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.2122e-03, 0.0000e+00, 6.1315e-03, 3.3543e-03,\n",
       "          0.0000e+00, 2.7930e-03, 1.2358e-04, 2.4600e-03, 0.0000e+00, 2.8141e-03,\n",
       "          0.0000e+00, 0.0000e+00, 1.9688e-03, 2.3209e-03, 1.5243e-03, 0.0000e+00,\n",
       "          0.0000e+00, 4.1076e-03, 1.4223e-03, 8.5112e-04, 0.0000e+00, 0.0000e+00,\n",
       "          1.0367e-03, 5.7510e-04, 0.0000e+00, 0.0000e+00, 2.7268e-04, 2.4055e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6776e-04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4181e-03,\n",
       "          0.0000e+00, 2.2869e-03, 6.8801e-05, 1.7873e-03, 0.0000e+00, 0.0000e+00,\n",
       "          1.6718e-03, 4.3410e-03, 0.0000e+00, 6.3734e-04, 0.0000e+00, 0.0000e+00,\n",
       "          8.0266e-05, 1.4043e-03, 2.1870e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.5765e-03, 0.0000e+00, 2.6068e-03, 2.4793e-03, 0.0000e+00, 0.0000e+00,\n",
       "          1.3590e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2723e-03,\n",
       "          2.7335e-03, 2.5340e-03, 3.3681e-04, 3.4601e-04, 0.0000e+00, 0.0000e+00,\n",
       "          4.1564e-05, 0.0000e+00, 5.3992e-04, 1.0752e-03, 1.9787e-03, 4.2797e-04,\n",
       "          0.0000e+00, 1.9904e-03, 2.6647e-03, 0.0000e+00, 2.5186e-03, 6.0603e-04,\n",
       "          1.3274e-03, 0.0000e+00, 0.0000e+00, 3.3582e-03, 0.0000e+00, 1.5114e-03,\n",
       "          1.0745e-03, 0.0000e+00, 3.1494e-03, 0.0000e+00, 3.8912e-03, 0.0000e+00,\n",
       "          7.0023e-04, 0.0000e+00, 1.2900e-03, 2.6482e-03, 2.5873e-03, 0.0000e+00,\n",
       "          0.0000e+00, 2.5905e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1493e-03,\n",
       "          1.3603e-03, 0.0000e+00, 4.4565e-04, 0.0000e+00, 1.7654e-04, 0.0000e+00,\n",
       "          0.0000e+00, 4.6051e-03, 8.7623e-04, 2.1208e-03, 3.0542e-03, 0.0000e+00,\n",
       "          0.0000e+00, 3.9709e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.8970e-03, 3.5383e-03, 1.2197e-03, 0.0000e+00, 7.8965e-04, 2.0059e-03,\n",
       "          3.4614e-03, 1.0423e-03, 0.0000e+00, 3.3637e-03, 0.0000e+00, 1.4287e-03,\n",
       "          3.8925e-03, 1.9893e-04, 0.0000e+00, 3.6857e-04, 8.0253e-04, 1.7705e-05,\n",
       "          1.6057e-03, 0.0000e+00, 5.7961e-03, 1.4298e-03, 0.0000e+00, 0.0000e+00,\n",
       "          3.7447e-05, 4.1586e-03, 0.0000e+00, 1.5170e-04, 4.5764e-04, 5.5134e-04,\n",
       "          4.3007e-03, 4.4549e-04, 2.5645e-03, 0.0000e+00, 0.0000e+00, 3.5590e-04,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5407e-03, 4.8162e-03, 2.4851e-03,\n",
       "          1.3103e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4197e-03, 0.0000e+00,\n",
       "          3.4236e-04, 0.0000e+00, 2.5178e-03, 0.0000e+00, 2.8348e-03, 2.9131e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5359e-03,\n",
       "          0.0000e+00, 7.9878e-04, 0.0000e+00, 9.0024e-04, 1.2539e-03, 6.7790e-04,\n",
       "          2.5139e-03, 5.6261e-03, 0.0000e+00, 1.3672e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.2412e-03, 0.0000e+00, 5.5428e-03, 0.0000e+00,\n",
       "          1.3909e-03, 1.9412e-03, 4.4913e-05, 0.0000e+00, 3.4032e-03, 0.0000e+00,\n",
       "          3.6376e-03, 2.5643e-03, 0.0000e+00, 1.7814e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.7335e-03, 0.0000e+00, 2.1939e-03, 0.0000e+00,\n",
       "          3.6270e-04, 0.0000e+00, 2.8984e-03, 1.1690e-03, 1.8445e-03, 3.0015e-03,\n",
       "          0.0000e+00, 0.0000e+00, 1.9867e-03, 2.6700e-03, 2.0423e-03, 1.9038e-03,\n",
       "          0.0000e+00, 0.0000e+00, 3.5696e-03, 0.0000e+00],\n",
       "         [0.0000e+00, 5.5985e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.2122e-03, 0.0000e+00, 6.1315e-03, 3.3543e-03,\n",
       "          0.0000e+00, 2.7930e-03, 1.2358e-04, 2.4600e-03, 0.0000e+00, 2.8141e-03,\n",
       "          0.0000e+00, 0.0000e+00, 1.9688e-03, 2.3209e-03, 1.5243e-03, 0.0000e+00,\n",
       "          0.0000e+00, 4.1076e-03, 1.4223e-03, 8.5112e-04, 0.0000e+00, 0.0000e+00,\n",
       "          1.0367e-03, 5.7510e-04, 0.0000e+00, 0.0000e+00, 2.7268e-04, 2.4055e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6776e-04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4181e-03,\n",
       "          0.0000e+00, 2.2869e-03, 6.8801e-05, 1.7873e-03, 0.0000e+00, 0.0000e+00,\n",
       "          1.6718e-03, 4.3410e-03, 0.0000e+00, 6.3734e-04, 0.0000e+00, 0.0000e+00,\n",
       "          8.0266e-05, 1.4043e-03, 2.1870e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.5765e-03, 0.0000e+00, 2.6068e-03, 2.4793e-03, 0.0000e+00, 0.0000e+00,\n",
       "          1.3590e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2723e-03,\n",
       "          2.7335e-03, 2.5340e-03, 3.3681e-04, 3.4601e-04, 0.0000e+00, 0.0000e+00,\n",
       "          4.1564e-05, 0.0000e+00, 5.3992e-04, 1.0752e-03, 1.9787e-03, 4.2797e-04,\n",
       "          0.0000e+00, 1.9904e-03, 2.6647e-03, 0.0000e+00, 2.5186e-03, 6.0603e-04,\n",
       "          1.3274e-03, 0.0000e+00, 0.0000e+00, 3.3582e-03, 0.0000e+00, 1.5114e-03,\n",
       "          1.0745e-03, 0.0000e+00, 3.1494e-03, 0.0000e+00, 3.8912e-03, 0.0000e+00,\n",
       "          7.0023e-04, 0.0000e+00, 1.2900e-03, 2.6482e-03, 2.5873e-03, 0.0000e+00,\n",
       "          0.0000e+00, 2.5905e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1493e-03,\n",
       "          1.3603e-03, 0.0000e+00, 4.4565e-04, 0.0000e+00, 1.7654e-04, 0.0000e+00,\n",
       "          0.0000e+00, 4.6051e-03, 8.7623e-04, 2.1208e-03, 3.0542e-03, 0.0000e+00,\n",
       "          0.0000e+00, 3.9709e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.8970e-03, 3.5383e-03, 1.2197e-03, 0.0000e+00, 7.8965e-04, 2.0059e-03,\n",
       "          3.4614e-03, 1.0423e-03, 0.0000e+00, 3.3637e-03, 0.0000e+00, 1.4287e-03,\n",
       "          3.8925e-03, 1.9893e-04, 0.0000e+00, 3.6857e-04, 8.0253e-04, 1.7705e-05,\n",
       "          1.6057e-03, 0.0000e+00, 5.7961e-03, 1.4298e-03, 0.0000e+00, 0.0000e+00,\n",
       "          3.7447e-05, 4.1586e-03, 0.0000e+00, 1.5170e-04, 4.5764e-04, 5.5134e-04,\n",
       "          4.3007e-03, 4.4549e-04, 2.5645e-03, 0.0000e+00, 0.0000e+00, 3.5590e-04,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5407e-03, 4.8162e-03, 2.4851e-03,\n",
       "          1.3103e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4197e-03, 0.0000e+00,\n",
       "          3.4236e-04, 0.0000e+00, 2.5178e-03, 0.0000e+00, 2.8348e-03, 2.9131e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5359e-03,\n",
       "          0.0000e+00, 7.9878e-04, 0.0000e+00, 9.0024e-04, 1.2539e-03, 6.7790e-04,\n",
       "          2.5139e-03, 5.6261e-03, 0.0000e+00, 1.3672e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.2412e-03, 0.0000e+00, 5.5428e-03, 0.0000e+00,\n",
       "          1.3909e-03, 1.9412e-03, 4.4913e-05, 0.0000e+00, 3.4032e-03, 0.0000e+00,\n",
       "          3.6376e-03, 2.5643e-03, 0.0000e+00, 1.7814e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.7335e-03, 0.0000e+00, 2.1939e-03, 0.0000e+00,\n",
       "          3.6270e-04, 0.0000e+00, 2.8984e-03, 1.1690e-03, 1.8445e-03, 3.0015e-03,\n",
       "          0.0000e+00, 0.0000e+00, 1.9867e-03, 2.6700e-03, 2.0423e-03, 1.9038e-03,\n",
       "          0.0000e+00, 0.0000e+00, 3.5696e-03, 0.0000e+00]],\n",
       "        grad_fn=<DifferentiableGraphBackward>)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online(data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
